---
title: "SEM"
author: "Kat Lopez"
date: "October 31, 2017"
output:
  html_document: default
  pdf_document: default
---
```{r, echo=FALSE, message=F, warning=F, error=F, include=FALSE}
PDS_stats= read.csv("~/Dropbox/3rd Fall/Longitudinal data analysis/STATS_resting_state_BRIEF.csv")
#this converts variables to numeric starting from column 4
id <- c(1,4:ncol(PDS_stats))
PDS_stats[,id] <- as.numeric(as.character(unlist(PDS_stats[,id])))
library(lavaan)
library(semTools)
library(semPlot)
library(lme4)
library(tidyr)
library(ggplot2)

#convert variables
PDS_stats$T3gecrs_combined_conv=PDS_stats$T3gecrs_Combined/100
PDS_stats$T12gecrs_conv=PDS_stats$T12gecrs/100
PDS_stats$T14gecrs_conv=PDS_stats$T14gecrs/100
```

1) Fit a measurement model to your constructs at one time point. Try out the different types of scaling discussed in class. What changes what stays the same?

```{r, echo=TRUE, warning=F, error=F}
metacognition.index.T3= ' metacognition.index.T3 =~ T3initirs + T3wrkmrs + T3plorgrs +T3orgmars + T3monirs' 
# Fixed factor
fixed<- cfa(metacognition.index.T3, missing= "ML", PDS_stats, std.lv = T)
summary(fixed, fit.measures= TRUE, standardized= TRUE)
semPaths(fixed, what = "paths", whatLabels= "est", layout = "tree")


# Marker variable
marker<- cfa(metacognition.index.T3, data=PDS_stats, missing= "ML")
summary (marker, fit.measures=TRUE, standardized= TRUE)
semPaths(marker, what = "paths", whatLabels= "est", layout = "tree")

# all fit statistics are the same for both models
#Comparative Fit Index (CFI)                    0.977
#Tucker-Lewis Index (TLI)                       0.955
#RMSEA                                          0.133
#SRMR                                           0.026
# All parameters stay the same with the exception of latent variable and variances estimates, which is expected as constraints differ between models-->  Fixed factor: constrains variance of latent factor to 1; Marker variable: fixes first factor loading to 1
```

2) What do the fit statistics say about your latent variable? Good/bad? Is your latent variable Just identified/saturated, under identified or over identified?

RMSEA or SRMR; Above .10 poor fit Below .08 acceptable
CFI, TFI; Usually >.9 is okay. Some care about > .95  
```{r, echo=TRUE, warning=F, error=F}

# All fit statistics (with the exception of RMSEA) suggest good fit of the latent variable
#Comparative Fit Index (CFI)                    0.977
#Tucker-Lewis Index (TLI)                       0.955
#RMSEA                                          0.133
#SRMR                                           0.026

# two below compare the fit of your model to the fit of the null model
  # CFI= 0.982 # considered to be good fit
  # TLI= 0.947 # considered to be good fit

# df= knowns-knowns; DF=5. This would  suggest that the latent variable is overidentified and thus we are able to assess the fit of the model and estimate the unknown parameters. However, p value of chi square is barely significant (p=0.052)
```

3) Fit a longitudinal CFA model where you a) first correlate your latent factors across time and then b) a second model that predicts later times by a prevous time (ie auto regressive; t1 -> t2 -> t3). What are your conclusions? How does one differ from the other?

```{r, echo=TRUE, warning=F, error=F}
Full.mod<- '
metacognition.index.T3 =~ T3initirs + T3wrkmrs + T3plorgrs +T3orgmars + T3monirs 
metacognition.index.T12 =~ T12initirs + T12wrkmrs + T12plorgrs +T12orgmars + T12monirs 
metacognition.index.T14 =~ T14initirs + T14wrkmrs + T14plorgrs +T14orgmars + T14monirs 
'

test<- cfa(Full.mod, data=PDS_stats, std.lv=TRUE, missing= "ML") 
summary (test, fit.measures=TRUE, standardized= TRUE)
semPaths(test, what = "paths", whatLabels= "est", layout = "tree")
# Covariances indicate that my latent variable at time 3 is moderately correlated at time 12(0.598) and 14 (0.493), and that my latent variable at time 12 is highly correlated with time 14 (0.823). All covariances are significant 

aut0regressive<- '
metacognition.index.T3 =~ T3initirs + T3wrkmrs + T3plorgrs +T3orgmars + T3monirs 
metacognition.index.T12 =~ T12initirs + T12wrkmrs + T12plorgrs +T12orgmars + T12monirs 
metacognition.index.T14 =~ T14initirs + T14wrkmrs + T14plorgrs +T14orgmars + T14monirs 
metacognition.index.T14 ~ metacognition.index.T12
metacognition.index.T12 ~ metacognition.index.T3
'

autoreg<- cfa(aut0regressive, std.lv=TRUE, data=PDS_stats, missing= "ML") 
summary (autoreg, fit.measures=TRUE, standardized= TRUE)
semPaths(autoreg, what = "paths", whatLabels= "est", layout = "tree")
# Covariances:
# metacognition.index.T3 ~~                                     
#    mtcgntn.nd.T12              0.598    0.103    5.826    0.000
#    mtcgntn.nd.T14              0.493    0.164    3.007    0.003
# metacognition.index.T12 ~~                                    
#    mtcgntn.nd.T14              0.823    0.039   20.903    0.000

# Regressions
# metacognition.index.T14 ~                                    
#    mtcgntn.nd.T12             1.162    0.197    5.900    0.000
#  metacognition.index.T12 ~                                    
#    mtcgntn.ndx.T3             0.747    0.194    3.844    0.000

# shows similar pattern; metacognition index at time 3 is highly related to time 12, which shows an even higher relationship to time 14. Greater relationship between 12 and 14 relative to the relationship between 3 and 12 may be due to the time interval between data acquisition (i.e. Time between  12 and 14 are closer in proximity)
```

4) Fit a longitdinal growth model in SEM and in HLM. Compare and contrast the differences.

```{r, echo=TRUE, warning=F, error=F}
#prep data for HLM
    PDS <- read.csv("~/Dropbox/3rd Fall/Longitudinal data analysis/PDS_II_LI_reduced_09_01_17.csv")
library(plyr)
    
    ## renaming variables
    PDS <- rename(PDS, c("Subid_fMRI"= "ID"))
    PDS <- rename(PDS, c("CON_CON_WAVE1"= "CONCON_WAVE1"))
    PDS <- rename(PDS, c("CON_CON_WAVE2"= "CONCON_WAVE2"))
    PDS <- rename(PDS, c("CON_CON_WAVE3"= "CONCON_WAVE3"))
    PDS <- rename(PDS, c("CON_FPN_WAVE1"= "CONFPN_WAVE1"))
    PDS <- rename(PDS, c("CON_FPN_WAVE2"= "CONFPN_WAVE2"))
    PDS <- rename(PDS, c("CON_FPN_WAVE3"= "CONFPN_WAVE3"))

    ## randomly created DOB and added to dataframe
    Agenow=sample(seq(as.Date('2000/01/01'), as.Date('2012/01/01'), by='day'), 136)
    DOBDOB_WAVE1=Agenow
    
    #Centers age to be able to interpret 0
    PDS1=PDS
    Agecen_WAVE1= PDS$Ageage_WAVE1- mean(PDS1$Ageage_WAVE1)
    Agecen_WAVE2= PDS$Ageage_WAVE2- mean(PDS1$Ageage_WAVE2, na.rm=TRUE)
    Agecen_WAVE3= PDS$Ageage_WAVE3- mean(PDS1$Ageage_WAVE3)
    PDS1=cbind(PDS, Agecen_WAVE1, Agecen_WAVE2, Agecen_WAVE3, DOBDOB_WAVE1)
    ## converting to long format
    wide_to_long_new <- PDS1 %>%
    gather(-DOBDOB_WAVE1,-ID,-Sex, key = "Timepoint", value = "Value") %>%
    separate(Timepoint, into = c("var", "omit", "wave"), sep = c(6, 11)) %>%
    spread(key = var, value = Value)

#HLM growth model fixed slope; intercept=-0.029438, slope= -0.002792
HLM<- lmer(CONFPN ~ Agecen+ (1|ID), data=wide_to_long_new)
summary(HLM)
#HLM growth model random slope; intercept=-0.029639, slope= -0.002333
HLM2<- lmer(CONFPN ~ Agecen+ (Agecen|ID), data=wide_to_long_new)
summary(HLM2)
anova(HLM, HLM2)
# BIC is smaller in HLM model using fixed slope but not by much. Additionally, P was not significant. This would suggest that adding a random slope does not add to the model. 

# SEM growth model with 3 timepoints intercept=-0.030 and slope=0.001 with fixed coefficients  
# with a fixed slope
  fixed.slope.network= ' i=~ 1*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 1*WAVE3_FPN_CON
  s=~ 0*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 2*WAVE3_FPN_CON
  s ~~ 0*s'  #fixes slope
  fixed.slope.network.fit= growth(fixed.slope.network, data = PDS_stats, missing= "ML")
  inspect(fixed.slope.network.fit, "cov.lv")
  summary (fixed.slope.network.fit, fit.measures=TRUE, standardized=TRUE)
  # intercept= -0.030, slope= 0.001
  # RMSEA= 0.000; SRMR= 0.021, both suggesting acceptable fits
  # CFI= 1.000; TFI= 1.130, suggesting larger distance from the worse fit (null), larger distance than intercept only
  semPaths(fixed.slope.network.fit, what = "paths", whatLabels= "est", layout = "tree")
      # Intercepts are nearly identical, HLM model has slightly smaller & negative slope.
      # Overall, Intercept between SEM and HLM were largely the same, slopes were similar as well. 
# with a random slope
  random.slope.network= ' i=~ 1*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 1*WAVE3_FPN_CON
  s=~ 0*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 2*WAVE3_FPN_CON'
  random.slope.network.fit= growth(random.slope.network, data = PDS_stats, missing= "ML")
  summary (random.slope.network.fit, fit.measures=TRUE, standardized=TRUE)
  # Intercept and slope were exactly the same as the fixed.slope.network.fit. This suggest that introducing a random slope in HLM did not change these figures. There was also no significant differences between HLM and SEM in fitting a random slope
  # RMSEA= 0.000; SRMR= 0.019, suggesting acceptable fits
  # CFI= 1.000; TFI= 1.079, suggesting larger distance from the worse fit (null), although slightly smaller distance than fixed slope
  semPaths(random.slope.network.fit, what = "paths", whatLabels= "est", layout = "tree")

  
# compare models 
  anova(fixed.slope.network.fit, random.slope.network.fit) #no significant differences, adding a random slope does not benefit the model (just like in SEM)
  
# Take away: Children average starting value for FPNCON connectivity is different from zero (negative) but do not show significant differences in starting point and change over time
```

5) Constrain the residual variances to be equal. Does this change the fit of your model? 

```{r, echo=TRUE, warning=F, error=F}
constrain.var<- ' i=~ 1*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 1*WAVE3_FPN_CON
s=~ 0*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 2*WAVE3_FPN_CON
WAVE1_FPN_CON~~ a*WAVE1_FPN_CON
WAVE2_FPN_CON~~ a*WAVE2_FPN_CON
WAVE3_FPN_CON~~ a*WAVE3_FPN_CON
'
constrain.var1<- growth(constrain.var, missing= "ML", data = PDS_stats)
summary(constrain.var1, fit.measures=TRUE, standardized=TRUE)
# Constraining the residual variances improves the fit statistics (RMSEA & SRMR) but lowers others (CFI & TFI). Overall, there is no remarkable change
# RMSEA= 0.021; SRMR= 0.069, suggesting acceptable fits
  # CFI= 0.985; TFI= 0.985, suggesting larger distance from the worse fit (null)
semPaths(constrain.var1, what = "paths", whatLabels= "est", layout = "tree")
```

6) Contrain your slope to be fixed, not random. How does this change your model?

```{r, echo=TRUE, warning=F, error=F}
 fixed.slope.network= ' i=~ 1*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 1*WAVE3_FPN_CON
  s=~ 0*WAVE1_FPN_CON + 1*WAVE2_FPN_CON + 2*WAVE3_FPN_CON
  s ~~ 0*s'  #fixes slope
  fixed.slope.network.fit= growth(fixed.slope.network, data = PDS_stats, missing= "ML")
  inspect(fixed.slope.network.fit, "cov.lv")
  summary (fixed.slope.network.fit, fit.measures=TRUE, standardized=TRUE)
  
# compare models 
anova(fixed.slope.network.fit, random.slope.network.fit) 
# constraining the slope does not significantly change my model  p=0.8194
```

8) Try a different type of estimation (see lavaan tutorial for details). How does that change your model?

```{r, echo=TRUE, warning=F, error=F}
fixed.slope.network.fit= growth(fixed.slope.network, data = PDS_stats, missing= "ML")
  inspect(fixed.slope.network.fit, "cov.lv")
  summary (fixed.slope.network.fit, fit.measures=TRUE, standardized=TRUE)
  
#"MLMVS": maximum likelihood estimation with robust standard errors and a mean- and variance adjusted test statistic (aka the Satterthwaite approach). For complete data only.
  fixed.slope.network.fit2= growth(fixed.slope.network,  estimator = "MLMVS", data = PDS_stats, missing= "ML")
  inspect(fixed.slope.network.fit2, "cov.lv")
  summary (fixed.slope.network.fit2, fit.measures=TRUE, standardized=TRUE)
  #First, it used only reduced my numbers of observations by half- n=74 reflects completed data at all time points. My fit statistics, intercept, and slope do not change very much. Residual cariance for  .WAVE1_FPN_CON changes slightly. Loglikelihood decreases by half from 630 to 359.340
```
